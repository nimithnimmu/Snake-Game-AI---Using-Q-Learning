{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb09cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -10, Epsilon: 0.9801495006250001\n",
      "Episode: 50, Total Reward: -10, Epsilon: 0.2457325055235537\n",
      "Episode: 100, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 150, Total Reward: 10, Epsilon: 0.00998645168764533\n",
      "Episode: 200, Total Reward: 20, Epsilon: 0.00998645168764533\n",
      "Episode: 250, Total Reward: 10, Epsilon: 0.00998645168764533\n",
      "Episode: 300, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 350, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 400, Total Reward: 20, Epsilon: 0.00998645168764533\n",
      "Episode: 450, Total Reward: 10, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_q_learning.mp4.\n",
      "Moviepy - Writing video snake_game_q_learning.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_q_learning.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_q_learning.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import VideoClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 4  # Small grid size for Q-learning\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 500\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "            reward = -10 if not alive else 0\n",
    "\n",
    "            if snake.positions[0] == food.position:\n",
    "                snake.grow()\n",
    "                food.spawn()\n",
    "                reward = 10  # Reward for eating food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        frames.append((np.copy(snake.positions), food.position))\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, size, duration=10):\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xticks(np.arange(-0.5, size, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, size, 1))\n",
    "        ax.grid(which=\"both\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        grid_display = np.zeros((size, size))\n",
    "        grid_display[frames[0][1]] = 1  # Mark food\n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        for pos in frames[current_frame_idx][0]:\n",
    "            grid_display[pos] = 0.5  # Mark snake's position\n",
    "        ax.imshow(grid_display, cmap=\"gray\", origin=\"upper\", extent=[0, size, size, 0])\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    video = VideoClip(make_frame, duration=duration)\n",
    "    video.write_videofile(\"snake_game_q_learning.mp4\", fps=30)\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 10-second video of the learned path\n",
    "create_video(frames, GRID_SIZE, duration=10)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_q_learning.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84909dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 0, Epsilon: 0.9801495006250001\n",
      "Episode: 50, Total Reward: -10, Epsilon: 0.10376060541355137\n",
      "Episode: 100, Total Reward: 0, Epsilon: 0.017420043796269234\n",
      "Episode: 150, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 200, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 250, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 300, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 350, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 400, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 450, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_better_graphics.mp4.\n",
      "Moviepy - Writing video snake_game_better_graphics.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_better_graphics.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Increase grid size for better visualization\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 500\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "            reward = -10 if not alive else 0\n",
    "\n",
    "            if snake.positions[0] == food.position:\n",
    "                snake.grow()\n",
    "                food.spawn()\n",
    "                reward = 10  # Reward for eating food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        frames.append((np.copy(snake.positions), food.position))\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, size, duration=10):\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    video = VideoClip(make_frame, duration=duration)\n",
    "    video.write_videofile(\"snake_game_better_graphics.mp4\", fps=10)\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 10-second video of the learned path\n",
    "create_video(frames, GRID_SIZE, duration=10)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71aa3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -10, Epsilon: 0.8603841919146962\n",
      "Episode: 50, Total Reward: -10, Epsilon: 0.08035439121179945\n",
      "Episode: 100, Total Reward: -10, Epsilon: 0.014471156162198668\n",
      "Episode: 150, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 200, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 250, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 300, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 350, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 400, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 450, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_better_graphics.mp4.\n",
      "Moviepy - Writing video snake_game_better_graphics.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_better_graphics.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Increase grid size for better visualization\n",
    "MAX_SNAKE_LENGTH = 8  # Set a maximum length for the snake\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 500\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "            reward = -10 if not alive else 0\n",
    "\n",
    "            if snake.positions[0] == food.position:\n",
    "                snake.grow()\n",
    "                food.spawn()\n",
    "                reward = 10  # Reward for eating food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        frames.append((np.copy(snake.positions), food.position))\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, size, duration=10):\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    video = VideoClip(make_frame, duration=duration)\n",
    "    video.write_videofile(\"snake_game_better_graphics.mp4\", fps=10)\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 10-second video of the learned path\n",
    "create_video(frames, GRID_SIZE, duration=10)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2cdbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -14, Epsilon: 0.918316468354365\n",
      "Episode: 50, Total Reward: -13, Epsilon: 0.07018314008827135\n",
      "Episode: 100, Total Reward: -14, Epsilon: 0.010137759008060509\n",
      "Episode: 150, Total Reward: -19, Epsilon: 0.00998645168764533\n",
      "Episode: 200, Total Reward: 13, Epsilon: 0.00998645168764533\n",
      "Episode: 250, Total Reward: -16, Epsilon: 0.00998645168764533\n",
      "Episode: 300, Total Reward: -14, Epsilon: 0.00998645168764533\n",
      "Episode: 350, Total Reward: -18, Epsilon: 0.00998645168764533\n",
      "Episode: 400, Total Reward: -1, Epsilon: 0.00998645168764533\n",
      "Episode: 450, Total Reward: 1, Epsilon: 0.00998645168764533\n",
      "Episode: 500, Total Reward: -1, Epsilon: 0.00998645168764533\n",
      "Episode: 550, Total Reward: -17, Epsilon: 0.00998645168764533\n",
      "Episode: 600, Total Reward: 16, Epsilon: 0.00998645168764533\n",
      "Episode: 650, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Episode: 700, Total Reward: -3, Epsilon: 0.00998645168764533\n",
      "Episode: 750, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Episode: 800, Total Reward: -16, Epsilon: 0.00998645168764533\n",
      "Episode: 850, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 900, Total Reward: -1, Epsilon: 0.00998645168764533\n",
      "Episode: 950, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_better_graphics.mp4.\n",
      "Moviepy - Writing video snake_game_better_graphics.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_better_graphics.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Increase grid size for better visualization\n",
    "MAX_SNAKE_LENGTH = 8  # Set a maximum length for the snake\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 1000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        frames.append((np.copy(snake.positions), food.position))\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, size, duration=10):\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    video = VideoClip(make_frame, duration=duration)\n",
    "    video.write_videofile(\"snake_game_better_graphics.mp4\", fps=10)\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 10-second video of the learned path\n",
    "create_video(frames, GRID_SIZE, duration=10)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5d0480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -8, Epsilon: 0.9369146928798039\n",
      "Episode: 100, Total Reward: -8, Epsilon: 0.010499784796482848\n",
      "Episode: 200, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Episode: 300, Total Reward: -8, Epsilon: 0.00998645168764533\n",
      "Episode: 400, Total Reward: -5, Epsilon: 0.00998645168764533\n",
      "Episode: 500, Total Reward: -17, Epsilon: 0.00998645168764533\n",
      "Episode: 600, Total Reward: 14, Epsilon: 0.00998645168764533\n",
      "Episode: 700, Total Reward: 14, Epsilon: 0.00998645168764533\n",
      "Episode: 800, Total Reward: 4, Epsilon: 0.00998645168764533\n",
      "Episode: 900, Total Reward: -1, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 1100, Total Reward: 11, Epsilon: 0.00998645168764533\n",
      "Episode: 1200, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Episode: 1300, Total Reward: 10, Epsilon: 0.00998645168764533\n",
      "Episode: 1400, Total Reward: 45, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 1600, Total Reward: -4, Epsilon: 0.00998645168764533\n",
      "Episode: 1700, Total Reward: 15, Epsilon: 0.00998645168764533\n",
      "Episode: 1800, Total Reward: 26, Epsilon: 0.00998645168764533\n",
      "Episode: 1900, Total Reward: 24, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_better_graphics.mp4.\n",
      "Moviepy - Writing video snake_game_better_graphics.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_better_graphics.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Increase grid size for better visualization\n",
    "MAX_SNAKE_LENGTH = 10  # Set a maximum length for the snake\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 2000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(200):  # Increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, size, duration=20):  # Increase video duration to 20 seconds\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    video = VideoClip(make_frame, duration=duration)\n",
    "    video.write_videofile(\"snake_game_better_graphics.mp4\", fps=10)\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path\n",
    "create_video(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34ad838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -33, Epsilon: 0.6180388156137953\n",
      "Episode: 100, Total Reward: -15, Epsilon: 0.00998645168764533\n",
      "Episode: 200, Total Reward: -15, Epsilon: 0.00998645168764533\n",
      "Episode: 300, Total Reward: -10, Epsilon: 0.00998645168764533\n",
      "Episode: 400, Total Reward: 6, Epsilon: 0.00998645168764533\n",
      "Episode: 500, Total Reward: -17, Epsilon: 0.00998645168764533\n",
      "Episode: 600, Total Reward: -5, Epsilon: 0.00998645168764533\n",
      "Episode: 700, Total Reward: -9, Epsilon: 0.00998645168764533\n",
      "Episode: 800, Total Reward: 4, Epsilon: 0.00998645168764533\n",
      "Episode: 900, Total Reward: -4, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: -12, Epsilon: 0.00998645168764533\n",
      "Episode: 1100, Total Reward: -7, Epsilon: 0.00998645168764533\n",
      "Episode: 1200, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 1300, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 1400, Total Reward: 1, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 15, Epsilon: 0.00998645168764533\n",
      "Episode: 1600, Total Reward: 0, Epsilon: 0.00998645168764533\n",
      "Episode: 1700, Total Reward: 29, Epsilon: 0.00998645168764533\n",
      "Episode: 1800, Total Reward: 18, Epsilon: 0.00998645168764533\n",
      "Episode: 1900, Total Reward: 27, Epsilon: 0.00998645168764533\n",
      "Episode: 2000, Total Reward: 76, Epsilon: 0.00998645168764533\n",
      "Episode: 2100, Total Reward: 45, Epsilon: 0.00998645168764533\n",
      "Episode: 2200, Total Reward: 14, Epsilon: 0.00998645168764533\n",
      "Episode: 2300, Total Reward: 20, Epsilon: 0.00998645168764533\n",
      "Episode: 2400, Total Reward: 17, Epsilon: 0.00998645168764533\n",
      "Episode: 2500, Total Reward: 22, Epsilon: 0.00998645168764533\n",
      "Episode: 2600, Total Reward: 3, Epsilon: 0.00998645168764533\n",
      "Episode: 2700, Total Reward: 3, Epsilon: 0.00998645168764533\n",
      "Episode: 2800, Total Reward: 49, Epsilon: 0.00998645168764533\n",
      "Episode: 2900, Total Reward: 63, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_better_graphics.mp4.\n",
      "Moviepy - Writing video snake_game_better_graphics.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_better_graphics.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Increase grid size for better visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 3000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(300):  # Increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, size, duration=15):  # Keep video duration short but faster speed\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    video = VideoClip(make_frame, duration=duration)\n",
    "    video.write_videofile(\"snake_game_better_graphics.mp4\", fps=20)  # Speed up video by increasing fps\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 15-second video of the learned path\n",
    "create_video(frames, GRID_SIZE, duration=15)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_better_graphics.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cef619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -26, Epsilon: 0.8560822709551227\n",
      "Episode: 500, Total Reward: -19, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 3, Epsilon: 0.00998645168764533\n",
      "Episode: 2000, Total Reward: 11, Epsilon: 0.00998645168764533\n",
      "Episode: 2500, Total Reward: -1, Epsilon: 0.00998645168764533\n",
      "Episode: 3000, Total Reward: 18, Epsilon: 0.00998645168764533\n",
      "Episode: 3500, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 4000, Total Reward: 15, Epsilon: 0.00998645168764533\n",
      "Episode: 4500, Total Reward: 80, Epsilon: 0.00998645168764533\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 210\u001b[0m\n\u001b[0;32m    207\u001b[0m frames \u001b[38;5;241m=\u001b[39m simulate_agent(agent)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Create a 20-second video of the learned path with title\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m create_video_with_title(frames, GRID_SIZE, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Display video in notebook\u001b[39;00m\n\u001b[0;32m    213\u001b[0m HTML(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124m<video width=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m320\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m height=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m240\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m controls>\u001b[39m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124m  <source src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnake_game_with_title.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo/mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124m</video>\u001b[39m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 200\u001b[0m, in \u001b[0;36mcreate_video_with_title\u001b[1;34m(frames, size, duration)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Create video clips\u001b[39;00m\n\u001b[0;32m    199\u001b[0m title_clip \u001b[38;5;241m=\u001b[39m VideoClip(make_title_frame, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Title frame for 3 seconds\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m game_clip \u001b[38;5;241m=\u001b[39m VideoClip(make_frame, duration\u001b[38;5;241m=\u001b[39mduration \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Rest of the video for gameplay\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Concatenate title and gameplay clips\u001b[39;00m\n\u001b[0;32m    203\u001b[0m final_clip \u001b[38;5;241m=\u001b[39m concatenate_videoclips([title_clip, game_clip])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:86\u001b[0m, in \u001b[0;36mVideoClip.__init__\u001b[1;34m(self, make_frame, ismask, duration, has_constant_size)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_frame:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_frame \u001b[38;5;241m=\u001b[39m make_frame\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mismask \u001b[38;5;241m=\u001b[39m ismask\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_constant_size\u001b[38;5;241m=\u001b[39mhas_constant_size\n",
      "File \u001b[1;32m<decorator-gen-29>:2\u001b[0m, in \u001b[0;36mget_frame\u001b[1;34m(self, t)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\moviepy\\decorators.py:89\u001b[0m, in \u001b[0;36mpreprocess_args.<locals>.wrapper\u001b[1;34m(f, *a, **kw)\u001b[0m\n\u001b[0;32m     85\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name \u001b[38;5;129;01min\u001b[39;00m varnames) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[0;32m     86\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[0;32m     87\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m varnames \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m     88\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m kw\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mnew_a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\moviepy\\Clip.py:93\u001b[0m, in \u001b[0;36mClip.get_frame\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m frame\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_frame(t)\n",
      "Cell \u001b[1;32mIn[7], line 180\u001b[0m, in \u001b[0;36mcreate_video_with_title.<locals>.make_frame\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    177\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_yticklabels([])\n\u001b[0;32m    179\u001b[0m current_frame_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(t \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m/\u001b[39m duration), \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m snake_positions, food_position \u001b[38;5;241m=\u001b[39m frames[current_frame_idx]\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Draw the snake\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m snake_positions:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "MIN_SNAKE_LENGTH_FOR_VIDEO = 5  # Minimum length before capturing in video\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 5000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(500):  # Increase the number of steps in simulation\n",
    "        if snake.length >= MIN_SNAKE_LENGTH_FOR_VIDEO:\n",
    "            frames.append((np.copy(snake.positions), food.position))  # Start capturing frames once snake is long enough\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        current_frame_idx = min(int(t * len(frames) / duration), len(frames) - 1)\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00124994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -12, Epsilon: 0.8475428503023453\n",
      "Episode: 500, Total Reward: 6, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: -8, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 18, Epsilon: 0.00998645168764533\n",
      "Episode: 2000, Total Reward: 85, Epsilon: 0.00998645168764533\n",
      "Episode: 2500, Total Reward: 3, Epsilon: 0.00998645168764533\n",
      "Episode: 3000, Total Reward: 39, Epsilon: 0.00998645168764533\n",
      "Episode: 3500, Total Reward: 92, Epsilon: 0.00998645168764533\n",
      "Episode: 4000, Total Reward: 26, Epsilon: 0.00998645168764533\n",
      "Episode: 4500, Total Reward: 3, Epsilon: 0.00998645168764533\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 211\u001b[0m\n\u001b[0;32m    208\u001b[0m frames \u001b[38;5;241m=\u001b[39m simulate_agent(agent)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Create a 20-second video of the learned path with title\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m create_video_with_title(frames, GRID_SIZE, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Display video in notebook\u001b[39;00m\n\u001b[0;32m    214\u001b[0m HTML(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124m<video width=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m320\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m height=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m240\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m controls>\u001b[39m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124m  <source src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnake_game_with_title.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo/mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124m</video>\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 201\u001b[0m, in \u001b[0;36mcreate_video_with_title\u001b[1;34m(frames, size, duration)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Create video clips\u001b[39;00m\n\u001b[0;32m    200\u001b[0m title_clip \u001b[38;5;241m=\u001b[39m VideoClip(make_title_frame, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Title frame for 3 seconds\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m game_clip \u001b[38;5;241m=\u001b[39m VideoClip(make_frame, duration\u001b[38;5;241m=\u001b[39mduration \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Rest of the video for gameplay\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Concatenate title and gameplay clips\u001b[39;00m\n\u001b[0;32m    204\u001b[0m final_clip \u001b[38;5;241m=\u001b[39m concatenate_videoclips([title_clip, game_clip])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:86\u001b[0m, in \u001b[0;36mVideoClip.__init__\u001b[1;34m(self, make_frame, ismask, duration, has_constant_size)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_frame:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_frame \u001b[38;5;241m=\u001b[39m make_frame\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mismask \u001b[38;5;241m=\u001b[39m ismask\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_constant_size\u001b[38;5;241m=\u001b[39mhas_constant_size\n",
      "File \u001b[1;32m<decorator-gen-29>:2\u001b[0m, in \u001b[0;36mget_frame\u001b[1;34m(self, t)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\moviepy\\decorators.py:89\u001b[0m, in \u001b[0;36mpreprocess_args.<locals>.wrapper\u001b[1;34m(f, *a, **kw)\u001b[0m\n\u001b[0;32m     85\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name \u001b[38;5;129;01min\u001b[39;00m varnames) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[0;32m     86\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[0;32m     87\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m varnames \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m     88\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m kw\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mnew_a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\moviepy\\Clip.py:93\u001b[0m, in \u001b[0;36mClip.get_frame\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m frame\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_frame(t)\n",
      "Cell \u001b[1;32mIn[8], line 172\u001b[0m, in \u001b[0;36mcreate_video_with_title.<locals>.make_frame\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    170\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(frames)\n\u001b[0;32m    171\u001b[0m current_frame_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(t \u001b[38;5;241m*\u001b[39m frame_count \u001b[38;5;241m/\u001b[39m (duration \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m)), frame_count \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust frame selection\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m snake_positions, food_position \u001b[38;5;241m=\u001b[39m frames[current_frame_idx]\n\u001b[0;32m    174\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m    175\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlim(\u001b[38;5;241m0\u001b[39m, size)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "MIN_SNAKE_LENGTH_FOR_VIDEO = 5  # Minimum length before capturing in video\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 5000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(500):  # Increase the number of steps in simulation\n",
    "        if snake.length >= MIN_SNAKE_LENGTH_FOR_VIDEO:\n",
    "            frames.append((np.copy(snake.positions), food.position))  # Start capturing frames once snake is long enough\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4c1c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -10, Epsilon: 0.8822202429488013\n",
      "Episode: 500, Total Reward: 4, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: 3, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Episode: 2000, Total Reward: 12, Epsilon: 0.00998645168764533\n",
      "Episode: 2500, Total Reward: 8, Epsilon: 0.00998645168764533\n",
      "Episode: 3000, Total Reward: 59, Epsilon: 0.00998645168764533\n",
      "Episode: 3500, Total Reward: 17, Epsilon: 0.00998645168764533\n",
      "Episode: 4000, Total Reward: 29, Epsilon: 0.00998645168764533\n",
      "Episode: 4500, Total Reward: 17, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_with_title.mp4.\n",
      "Moviepy - Writing video snake_game_with_title.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_with_title.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "MIN_SNAKE_LENGTH_FOR_VIDEO = 5  # Minimum length before capturing in video\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 5000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(500):  # Increase the number of steps in simulation\n",
    "        if snake.length >= MIN_SNAKE_LENGTH_FOR_VIDEO:\n",
    "            frames.append((np.copy(snake.positions), food.position))  # Start capturing frames once snake is long enough\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        if frame_count == 0:\n",
    "            return make_title_frame(t)  # Show title if no frames are captured\n",
    "\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05f551ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2775570277.py, line 191)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 191\u001b[1;36m\u001b[0m\n\u001b[1;33m    ax.add_patch(food_circle)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 5000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(500):  # Increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))  # Capture frames from the start\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        if frame_count == 0:\n",
    "            return make_title_frame(t)  # Show title if no frames are captured\n",
    "\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "            ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69177b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -11, Epsilon: 0.9137248860125932\n",
      "Episode: 500, Total Reward: 3, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: 5, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 1, Epsilon: 0.00998645168764533\n",
      "Episode: 2000, Total Reward: 51, Epsilon: 0.00998645168764533\n",
      "Episode: 2500, Total Reward: 21, Epsilon: 0.00998645168764533\n",
      "Episode: 3000, Total Reward: 40, Epsilon: 0.00998645168764533\n",
      "Episode: 3500, Total Reward: 15, Epsilon: 0.00998645168764533\n",
      "Episode: 4000, Total Reward: 14, Epsilon: 0.00998645168764533\n",
      "Episode: 4500, Total Reward: 2, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_with_title.mp4.\n",
      "Moviepy - Writing video snake_game_with_title.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_with_title.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 5000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(200):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 10  # Reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(500):  # Increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))  # Capture frames from the start\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive:\n",
    "            break\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        if frame_count == 0:\n",
    "            return make_title_frame(t)  # Show title if no frames are captured\n",
    "\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc92826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -27, Epsilon: 0.7183288830986236\n",
      "Episode: 500, Total Reward: 19, Epsilon: 0.00998645168764533\n",
      "Episode: 1000, Total Reward: 29, Epsilon: 0.00998645168764533\n",
      "Episode: 1500, Total Reward: 16, Epsilon: 0.00998645168764533\n",
      "Episode: 2000, Total Reward: 61, Epsilon: 0.00998645168764533\n",
      "Episode: 2500, Total Reward: 33, Epsilon: 0.00998645168764533\n",
      "Episode: 3000, Total Reward: 43, Epsilon: 0.00998645168764533\n",
      "Episode: 3500, Total Reward: 14, Epsilon: 0.00998645168764533\n",
      "Episode: 4000, Total Reward: 13, Epsilon: 0.00998645168764533\n",
      "Episode: 4500, Total Reward: 28, Epsilon: 0.00998645168764533\n",
      "Episode: 5000, Total Reward: 134, Epsilon: 0.00998645168764533\n",
      "Episode: 5500, Total Reward: 38, Epsilon: 0.00998645168764533\n",
      "Moviepy - Building video snake_game_with_title.mp4.\n",
      "Moviepy - Writing video snake_game_with_title.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_with_title.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "TARGET_SNAKE_LENGTH = 5  # Desired snake length for the video\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 6000  # Increase episodes to allow more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(300):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 20  # Increase reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(1000):  # Further increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))  # Capture frames from the start\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive or snake.length >= TARGET_SNAKE_LENGTH:\n",
    "            break  # Stop if the snake dies or reaches the target length\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        if frame_count == 0:\n",
    "            return make_title_frame(t)  # Show title if no frames are captured\n",
    "\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ff3bf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -15, Epsilon: 0.93603800616986\n",
      "Episode: 500, Total Reward: -16, Epsilon: 0.00999258133189499\n",
      "Episode: 1000, Total Reward: 11, Epsilon: 0.00999258133189499\n",
      "Episode: 1500, Total Reward: 11, Epsilon: 0.00999258133189499\n",
      "Episode: 2000, Total Reward: 5, Epsilon: 0.00999258133189499\n",
      "Episode: 2500, Total Reward: 37, Epsilon: 0.00999258133189499\n",
      "Episode: 3000, Total Reward: 93, Epsilon: 0.00999258133189499\n",
      "Episode: 3500, Total Reward: 60, Epsilon: 0.00999258133189499\n",
      "Episode: 4000, Total Reward: 106, Epsilon: 0.00999258133189499\n",
      "Episode: 4500, Total Reward: 14, Epsilon: 0.00999258133189499\n",
      "Episode: 5000, Total Reward: 10, Epsilon: 0.00999258133189499\n",
      "Episode: 5500, Total Reward: 61, Epsilon: 0.00999258133189499\n",
      "Episode: 6000, Total Reward: 57, Epsilon: 0.00999258133189499\n",
      "Episode: 6500, Total Reward: 85, Epsilon: 0.00999258133189499\n",
      "Episode: 7000, Total Reward: 40, Epsilon: 0.00999258133189499\n",
      "Episode: 7500, Total Reward: 10, Epsilon: 0.00999258133189499\n",
      "Moviepy - Building video snake_game_with_title.mp4.\n",
      "Moviepy - Writing video snake_game_with_title.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_with_title.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "TARGET_SNAKE_LENGTH = 5  # Desired snake length for the video\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.997  # Slightly slower decay for more exploration\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 8000  # Further increase episodes for more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(400):  # Increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 20  # Increase reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(1500):  # Further increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))  # Capture frames from the start\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive or snake.length >= TARGET_SNAKE_LENGTH:\n",
    "            break  # Stop if the snake dies or reaches the target length\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        if frame_count == 0:\n",
    "            return make_title_frame(t)  # Show title if no frames are captured\n",
    "\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ee5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 34, Epsilon: 0.9762739865836303\n",
      "Episode: 500, Total Reward: -17, Epsilon: 0.009998671593271896\n",
      "Episode: 1000, Total Reward: 37, Epsilon: 0.009998671593271896\n",
      "Episode: 1500, Total Reward: 36, Epsilon: 0.009998671593271896\n",
      "Episode: 2000, Total Reward: 145, Epsilon: 0.009998671593271896\n",
      "Episode: 2500, Total Reward: 37, Epsilon: 0.009998671593271896\n",
      "Episode: 3000, Total Reward: 149, Epsilon: 0.009998671593271896\n",
      "Episode: 3500, Total Reward: 98, Epsilon: 0.009998671593271896\n",
      "Episode: 4000, Total Reward: 42, Epsilon: 0.009998671593271896\n",
      "Episode: 4500, Total Reward: 101, Epsilon: 0.009998671593271896\n",
      "Episode: 5000, Total Reward: 92, Epsilon: 0.009998671593271896\n",
      "Episode: 5500, Total Reward: 93, Epsilon: 0.009998671593271896\n",
      "Episode: 6000, Total Reward: 249, Epsilon: 0.009998671593271896\n",
      "Episode: 6500, Total Reward: 142, Epsilon: 0.009998671593271896\n",
      "Episode: 7000, Total Reward: 198, Epsilon: 0.009998671593271896\n",
      "Episode: 7500, Total Reward: 146, Epsilon: 0.009998671593271896\n",
      "Episode: 8000, Total Reward: 105, Epsilon: 0.009998671593271896\n",
      "Episode: 8500, Total Reward: 95, Epsilon: 0.009998671593271896\n",
      "Episode: 9000, Total Reward: 92, Epsilon: 0.009998671593271896\n",
      "Episode: 9500, Total Reward: 260, Epsilon: 0.009998671593271896\n",
      "Episode: 10000, Total Reward: 96, Epsilon: 0.009998671593271896\n",
      "Episode: 10500, Total Reward: 96, Epsilon: 0.009998671593271896\n",
      "Episode: 11000, Total Reward: 141, Epsilon: 0.009998671593271896\n",
      "Episode: 11500, Total Reward: 39, Epsilon: 0.009998671593271896\n",
      "Moviepy - Building video snake_game_with_title.mp4.\n",
      "Moviepy - Writing video snake_game_with_title.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready snake_game_with_title.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from moviepy.editor import VideoClip, concatenate_videoclips\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Environment settings\n",
    "GRID_SIZE = 10  # Grid size for visualization\n",
    "MAX_SNAKE_LENGTH = 15  # Set a longer maximum length for the snake\n",
    "TARGET_SNAKE_LENGTH = 5  # Desired snake length for the video\n",
    "\n",
    "# Define the Snake and Food classes in a simple grid environment\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = [(GRID_SIZE // 2, GRID_SIZE // 2)]  # Start in the middle\n",
    "        self.direction = (0, -1)  # Initially moving up\n",
    "        self.length = 1\n",
    "\n",
    "    def move(self, action):\n",
    "        if action == 0:\n",
    "            self.direction = (0, -1)  # Move up\n",
    "        elif action == 1:\n",
    "            self.direction = (1, 0)  # Move right\n",
    "        elif action == 2:\n",
    "            self.direction = (0, 1)  # Move down\n",
    "        elif action == 3:\n",
    "            self.direction = (-1, 0)  # Move left\n",
    "\n",
    "        head_x, head_y = self.positions[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "        # Check for wall collision or self-collision\n",
    "        if new_head in self.positions or not (0 <= new_head[0] < GRID_SIZE) or not (0 <= new_head[1] < GRID_SIZE):\n",
    "            return False  # Game over\n",
    "\n",
    "        # Update positions\n",
    "        self.positions = [new_head] + self.positions[:self.length - 1]\n",
    "        return True\n",
    "\n",
    "    def grow(self):\n",
    "        if self.length < MAX_SNAKE_LENGTH:  # Grow only if below max length\n",
    "            self.length += 1  # Increase length by 1\n",
    "\n",
    "class Food:\n",
    "    def __init__(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "    def spawn(self):\n",
    "        self.position = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "\n",
    "# Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = (GRID_SIZE, GRID_SIZE, GRID_SIZE, GRID_SIZE)\n",
    "        self.action_size = 4  # 4 possible actions: up, right, down, left\n",
    "        self.q_table = np.zeros(self.state_size + (self.action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.95\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999  # Much slower decay for longer exploration\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head_x, head_y = snake.positions[0]\n",
    "        food_x, food_y = food.position\n",
    "        return head_x, head_y, food_x, food_y\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state + (best_next_action,)]\n",
    "        self.q_table[state + (action,)] += self.learning_rate * (td_target - self.q_table[state + (action,)])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Train the agent\n",
    "def train_q_learning():\n",
    "    agent = QLearningAgent()\n",
    "    episodes = 12000  # Significantly increase episodes for more learning\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        snake = Snake()\n",
    "        food = Food()\n",
    "        state = agent.get_state(snake, food)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(500):  # Further increase steps per episode\n",
    "            action = agent.act(state)\n",
    "            alive = snake.move(action)\n",
    "\n",
    "            # Calculate reward\n",
    "            if not alive:\n",
    "                reward = -10  # Strong negative reward for dying\n",
    "            else:\n",
    "                current_distance = np.abs(snake.positions[0][0] - food.position[0]) + np.abs(snake.positions[0][1] - food.position[1])\n",
    "                next_distance = np.abs(snake.positions[0][0] + snake.direction[0] - food.position[0]) + np.abs(snake.positions[0][1] + snake.direction[1] - food.position[1])\n",
    "                \n",
    "                if snake.positions[0] == food.position:\n",
    "                    reward = 50  # Further increase reward for eating food\n",
    "                    snake.grow()\n",
    "                    food.spawn()\n",
    "                elif next_distance < current_distance:\n",
    "                    reward = 1  # Small positive reward for moving closer to the food\n",
    "                else:\n",
    "                    reward = -1  # Small negative reward for moving away from the food\n",
    "\n",
    "            next_state = agent.get_state(snake, food)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if not alive:\n",
    "                break\n",
    "\n",
    "        # Print the episode result\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "agent = train_q_learning()\n",
    "\n",
    "# Function to simulate and create a video\n",
    "def simulate_agent(agent):\n",
    "    snake = Snake()\n",
    "    food = Food()\n",
    "    state = agent.get_state(snake, food)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(2000):  # Further increase the number of steps in simulation\n",
    "        frames.append((np.copy(snake.positions), food.position))  # Capture frames from the start\n",
    "        action = agent.act(state)\n",
    "        alive = snake.move(action)\n",
    "        if not alive or snake.length >= TARGET_SNAKE_LENGTH:\n",
    "            break  # Stop if the snake dies or reaches the target length\n",
    "        if snake.positions[0] == food.position:\n",
    "            snake.grow()\n",
    "            food.spawn()\n",
    "        state = agent.get_state(snake, food)\n",
    "\n",
    "    # Restart simulation if snake hasn't grown enough\n",
    "    if snake.length < TARGET_SNAKE_LENGTH:\n",
    "        return simulate_agent(agent)  # Recursively try again until it reaches the target length\n",
    "\n",
    "    return frames\n",
    "\n",
    "def create_video_with_title(frames, size, duration=20):  # Keep video duration to 20 seconds\n",
    "    # Create the title frame\n",
    "    def make_title_frame(t):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.text(0.5, 0.5, 'Snake Game AI\\nUsing Q-Learning', fontsize=24, ha='center', va='center', wrap=True)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create the gameplay frames\n",
    "    def make_frame(t):\n",
    "        frame_count = len(frames)\n",
    "        if frame_count == 0:\n",
    "            return make_title_frame(t)  # Show title if no frames are captured\n",
    "\n",
    "        current_frame_idx = min(int(t * frame_count / (duration - 3)), frame_count - 1)  # Adjust frame selection\n",
    "        snake_positions, food_position = frames[current_frame_idx]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_xlim(0, size)\n",
    "        ax.set_ylim(0, size)\n",
    "        ax.set_xticks(np.arange(0, size, 1))\n",
    "        ax.set_yticks(np.arange(0, size, 1))\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Draw the snake\n",
    "        for pos in snake_positions:\n",
    "            circle = patches.Circle((pos[1] + 0.5, size - pos[0] - 0.5), 0.3, color='green')\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "        # Draw the food\n",
    "        food_circle = patches.Circle((food_position[1] + 0.5, size - food_position[0] - 0.5), 0.3, color='red')\n",
    "        ax.add_patch(food_circle)\n",
    "\n",
    "        # Convert figure to image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "\n",
    "    # Create video clips\n",
    "    title_clip = VideoClip(make_title_frame, duration=3)  # Title frame for 3 seconds\n",
    "    game_clip = VideoClip(make_frame, duration=duration - 3)  # Rest of the video for gameplay\n",
    "\n",
    "    # Concatenate title and gameplay clips\n",
    "    final_clip = concatenate_videoclips([title_clip, game_clip])\n",
    "    final_clip.write_videofile(\"snake_game_with_title.mp4\", fps=30)  # Increase fps for faster video\n",
    "\n",
    "# Simulate the agent's learned behavior\n",
    "frames = simulate_agent(agent)\n",
    "\n",
    "# Create a 20-second video of the learned path with title\n",
    "create_video_with_title(frames, GRID_SIZE, duration=20)\n",
    "\n",
    "# Display video in notebook\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"snake_game_with_title.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c26fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
